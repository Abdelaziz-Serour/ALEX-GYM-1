{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jOWxz0Jztpfn"},"outputs":[],"source":["import os\n","import json\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from transformers import AutoImageProcessor, AutoModelForPreTraining"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1734906208869,"user":{"displayName":"ai research","userId":"07349987832510586956"},"user_tz":-120},"id":"ZDS-gRz_t4wO","outputId":"767b2b92-a233-4355-8069-4f355a2022a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA is not available. PyTorch is using the CPU.\n"]}],"source":["# Check if CUDA (GPU support) is available\n","if torch.cuda.is_available():\n","    print(\"CUDA is available. PyTorch can use the GPU.\")\n","    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n","else:\n","    print(\"CUDA is not available. PyTorch is using the CPU.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9100,"status":"ok","timestamp":1734906182050,"user":{"displayName":"ai research","userId":"07349987832510586956"},"user_tz":-120},"id":"iqEGFXzmus9S","outputId":"2eb48628-b7ce-47b3-b172-66e5d4fcc107"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# prompt: i want to access drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PQQ8kiErt-xy"},"outputs":[],"source":["num_frames = 16\n","image_size = 224\n","FeatureNum = 7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8Tzi0sst_9r"},"outputs":[],"source":["base_path = \"/content/drive/MyDrive/Vision_GYM_Research/Data\"\n","LOG_DIR = \"/content/drive/MyDrive/Vision_GYM_Research/tensorboard_logs\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7IrQLqZuXDS"},"outputs":[],"source":["def process_action_data(base_path, action_name):\n","    \"\"\"\n","    Process data for a specific action by loading the corresponding Excel and JSON files,\n","    adding pose data, and splitting into train, validation, and test sets.\n","\n","    Args:\n","        base_path (str): The base directory containing the files.\n","        action_name (str): The name of the action (e.g., 'squat', 'deadlift', 'lunges').\n","\n","    Returns:\n","        tuple: train_df, val_df, test_df DataFrames.\n","    \"\"\"\n","    # Load the Excel file\n","    excel_file = f\"{action_name}_edited.xlsx\"\n","    df = pd.read_excel(os.path.join(base_path, excel_file))\n","\n","    # Load the JSON files for front and lateral poses\n","    front_pose_file = f\"front_pose_{action_name}.json\"\n","    lat_pose_file = f\"lat_pose_{action_name}.json\"\n","\n","    def load_json_as_numpy(json_file):\n","        with open(json_file, 'r') as file:\n","            data = json.load(file)\n","        return np.array(data)\n","\n","    front_pose_array = load_json_as_numpy(os.path.join(base_path, front_pose_file))\n","    lat_pose_array = load_json_as_numpy(os.path.join(base_path, lat_pose_file))\n","\n","    # Ensure `front_pose` and `lat_pose` columns exist\n","    if 'front_pose' not in df.columns:\n","        df['front_pose'] = None\n","    if 'lat_pose' not in df.columns:\n","        df['lat_pose'] = None\n","\n","    # Assign the loaded arrays to the DataFrame if lengths match\n","    if len(front_pose_array) == len(df) and len(lat_pose_array) == len(df):\n","        df['front_pose'] = list(front_pose_array)\n","        df['lat_pose'] = list(lat_pose_array)\n","    else:\n","        raise ValueError(\"The length of the loaded arrays does not match the DataFrame.\")\n","\n","    # Shuffle the DataFrame\n","    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","    # Define split ratios\n","    train_ratio, val_ratio, test_ratio = 0.7, 0.15, 0.15\n","\n","    # Calculate the number of samples for each set\n","    total_samples = len(df)\n","    train_size = int(total_samples * train_ratio)\n","    val_size = int(total_samples * val_ratio)\n","\n","    # Split the DataFrame\n","    train_df = df.iloc[:train_size]\n","    val_df = df.iloc[train_size:train_size + val_size]\n","    test_df = df.iloc[train_size + val_size:]\n","\n","    return train_df, val_df, test_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGCaD_yducOq"},"outputs":[],"source":["train_df_squat, val_df_squat, test_df_squat = process_action_data(base_path, 'squat')\n","train_df_dead, val_df_dead, test_df_dead = process_action_data(base_path, 'deadlift')\n","train_df_lunge, val_df_lunge, test_df_lunge = process_action_data(base_path, 'lunges')\n","\n","\n","# Define the actions\n","actions = ['squat', 'deadlift', 'lunges']\n","\n","# Process data for each action and store results in dictionaries\n","splits = {action: process_action_data(base_path, action) for action in actions}\n","\n","# Concatenate and shuffle DataFrames for each split\n","train_df = pd.concat([splits[action][0] for action in actions]).sample(frac=1, random_state=1).reset_index(drop=True)\n","val_df = pd.concat([splits[action][1] for action in actions]).sample(frac=1, random_state=1).reset_index(drop=True)\n","test_df = pd.concat([splits[action][2] for action in actions]).sample(frac=1, random_state=1).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3AI2WSEDu6OG"},"outputs":[],"source":["import torchvision.models.video as models\n","\n","class PretrainedResNet3D(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super(PretrainedResNet3D, self).__init__()\n","        # Load the pretrained ResNet3D model\n","        self.resnet3d = models.r3d_18(pretrained=pretrained)\n","        # Replace the final fully connected layer with an identity layer\n","        self.resnet3d.fc = nn.Identity()\n","\n","        for name, param in self.resnet3d.named_parameters():\n","            if \"layer3\" not in name and \"layer4\" not in name:\n","                param.requires_grad = False\n","    def forward(self, x):\n","        return self.resnet3d(x)\n","\n","class DualInputResNet3D(nn.Module):\n","    def __init__(self, output_size= eFatureNum, feature_dim=512, dropout_rate=0.3):\n","        super(DualInputResNet3D, self).__init__()\n","        # Pretrained ResNet3D streams for frontal and lateral inputs\n","        self.resnet3d_frontal = PretrainedResNet3D()\n","        self.resnet3d_lateral = PretrainedResNet3D()\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(feature_dim * 2, 1024)  # Combine features from both inputs\n","        self.bn_fc1 = nn.BatchNorm1d(1024)\n","        self.fc2 = nn.Linear(1024, output_size)\n","\n","        # Activation and dropout\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, frontal, lateral):\n","        frontal = frontal.permute(0, 2, 1, 3, 4).contiguous()\n","        lateral = lateral.permute(0, 2, 1, 3, 4).contiguous()\n","\n","        # Process frontal input through ResNet3D\n","        x_f = self.resnet3d_frontal(frontal)\n","        x_f = x_f.view(x_f.size(0), -1)  # Flatten features\n","\n","        # Process lateral input through ResNet3D\n","        x_l = self.resnet3d_lateral(lateral)\n","        x_l = x_l.view(x_l.size(0), -1)  # Flatten features\n","\n","        # Concatenate features from frontal and lateral streams\n","        x = torch.cat((x_f, x_l), dim=1)\n","\n","        # Pass through fully connected layers\n","        x = self.relu(self.bn_fc1(self.fc1(x)))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkO2fOzJYd5o"},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","    \"\"\"\n","    A Residual Block with 1D convolutions, BatchNorm, and ReLU activations.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n","        self.bn1 = nn.BatchNorm1d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm1d(out_channels)\n","\n","        self.conv3 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.bn3 = nn.BatchNorm1d(out_channels)\n","\n","        # Shortcut connection\n","        self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride)\n","        self.shortcut_bn = nn.BatchNorm1d(out_channels)\n","\n","    def forward(self, x):\n","        shortcut = self.shortcut(x)\n","        shortcut = self.shortcut_bn(shortcut)\n","\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        x = self.conv3(x)\n","        x = self.bn3(x)\n","        x = self.relu(x)\n","\n","        x = x + shortcut\n","        x = self.relu(x)\n","        return x\n","\n","class Pose_Model(nn.Module):\n","    \"\"\"\n","    Pose Model using 1D Convolutions, Residual Blocks, and GRU for temporal aggregation.\n","    \"\"\"\n","    def __init__(self, input_channels=5984, residual_channels1=4096 , residual_channels2=1024, residual_channels3=512, final_channels=256, gru_hidden_size=128):\n","        super(Pose_Model, self).__init__()\n","\n","        # Initial 1D Convolution\n","        self.initial_conv = nn.Sequential(\n","            nn.Conv1d(input_channels, residual_channels1, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm1d(residual_channels1),\n","            nn.ReLU()\n","        )\n","\n","        # Two Residual Blocks\n","        self.residual_block1 = ResidualBlock(residual_channels1, residual_channels2, stride=2)\n","        self.residual_block2 = ResidualBlock(residual_channels2, residual_channels3, stride=2)\n","\n","        # # Adaptive Average Pooling\n","        # self.avg_pool = nn.AdaptiveAvgPool1d(1)\n","\n","        # Final 1D Convolution to reduce channels\n","        self.final_conv = nn.Sequential(\n","            nn.Conv1d(residual_channels3, final_channels, kernel_size=1),\n","            nn.BatchNorm1d(final_channels),\n","            nn.ReLU()\n","        )\n","\n","        # GRU for temporal aggregation\n","        self.gru = nn.GRU(input_size=final_channels, hidden_size=gru_hidden_size, batch_first=True)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: Pose landmarks tensor [batch_size, num_frames=16, input_channels=528]\n","        \"\"\"\n","        batch_size, num_frames, channels = x.shape\n","        x = x.permute(0, 2, 1)  # Reshape to [batch_size, input_channels, num_frames]\n","\n","        # Pass through the convolutional layers\n","        x = self.initial_conv(x)\n","        x = self.residual_block1(x)\n","        x = self.residual_block2(x)\n","        # x = self.avg_pool(x)  # Shape: [batch_size, residual_channels, 1]\n","\n","        x = self.final_conv(x)  # Shape: [batch_size, final_channels, 16]\n","\n","        # x = x.squeeze(-1)  # Shape: [batch_size, final_channels]\n","\n","        # Prepare for GRU\n","        x = x.permute(0, 2, 1)  # Reshape to [batch_size, 16, final_channels]\n","\n","        # GRU for temporal features\n","        _, pooled_features = self.gru(x)  # Shape: [1, batch_size, gru_hidden_size]\n","        return pooled_features.squeeze(0)  # Shape: [batch_size, gru_hidden_size]\n","\n","\n","class DualInputPose(nn.Module):\n","    def __init__(self, pose_input_channels=5984, hidden_size=128, output_size=7):\n","        super(DualInputPose, self).__init__()\n","\n","        # Pose models for front and lateral views\n","        self.front_model = Pose_Model(input_channels=pose_input_channels, gru_hidden_size=hidden_size)\n","        self.lat_model = Pose_Model(input_channels=pose_input_channels, gru_hidden_size=hidden_size)\n","\n","        # Fully connected layers for prediction\n","        self.fc = nn.Sequential(\n","            nn.Linear(hidden_size * 2, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(256, output_size)\n","        )\n","\n","    def forward(self, front_input, lat_input):\n","        \"\"\"\n","        front_input: Front view pose data [batch_size, num_frames=16, pose_input_channels=5984]\n","        lat_input: Lateral view pose data [batch_size, num_frames=16, pose_input_channels=5984]\n","        \"\"\"\n","        # Pass through the front and lateral Pose models\n","        front_features = self.front_model(front_input)  # Shape: [batch_size, hidden_size]\n","        lat_features = self.lat_model(lat_input)        # Shape: [batch_size, hidden_size]\n","\n","        # Concatenate features\n","        combined_features = torch.cat((front_features, lat_features), dim=1)  # Shape: [batch_size, hidden_size * 2]\n","\n","        # Predict criteria\n","        output = self.fc(combined_features)  # Shape: [batch_size, output_size]\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"azG5UlSmY07H"},"outputs":[],"source":["class MultiModalModel(nn.Module):\n","    \"\"\"\n","    Multi-modal model that combines pose-based and image-based inputs.\n","    \"\"\"\n","    def __init__(self,\n","                 pose_input_channels=5984,\n","                 image_feature_dim=512,\n","                 pose_hidden_size=128,\n","                 output_size=FeatureNum):\n","        super(MultiModalModel, self).__init__()\n","\n","        # Image model (DualInputResNet3D)\n","        self.vision_model = DualInputResNet3D(output_size=image_feature_dim)\n","\n","        # Pose model (DualInputPose)\n","        self.pose_model = DualInputPose(\n","            pose_input_channels=pose_input_channels,\n","            hidden_size=pose_hidden_size,\n","            output_size=pose_hidden_size\n","        )\n","\n","        # Fusion layer\n","        combined_feature_dim = image_feature_dim + pose_hidden_size\n","        self.fc = nn.Sequential(\n","            nn.Linear(combined_feature_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(256, output_size)\n","        )\n","\n","    def forward(self, image_frontal, image_lateral, pose_frontal, pose_lateral):\n","        \"\"\"\n","        Forward pass for multi-modal model.\n","\n","        Args:\n","        - image_frontal: Frontal view images [batch_size, num_frames=16, 3, 224, 224]\n","        - image_lateral: Lateral view images [batch_size, num_frames=16, 3, 224, 224]\n","        - pose_frontal: Frontal view pose data [batch_size, num_frames=16, pose_input_channels]\n","        - pose_lateral: Lateral view pose data [batch_size, num_frames=16, pose_input_channels]\n","\n","        Returns:\n","        - Output predictions [batch_size, output_size]\n","        \"\"\"\n","        # Extract image features\n","        image_features = self.vision_model(image_frontal, image_lateral)  # Shape: [batch_size, 512]\n","\n","        # Extract pose features\n","        pose_features = self.pose_model(pose_frontal, pose_lateral)  # Shape: [batch_size, 128]\n","\n","        # Concatenate features\n","        combined_features = torch.cat((image_features, pose_features), dim=1)  # Shape: [batch_size, combined_feature_dim]\n","\n","        # Pass through fusion layers\n","        output = self.fc(combined_features)  # Shape: [batch_size, output_size]\n","        return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bgne81SQViLP"},"outputs":[],"source":["def compute_pairwise_distances(points_tensor):\n","    \"\"\"\n","    Compute pairwise distances between 33 pose landmarks for each frame.\n","\n","    Args:\n","        points_tensor (torch.Tensor): Tensor of shape [num_frames, 33, 3],\n","                                       where each row is a point (x, y, z) for each frame.\n","\n","    Returns:\n","        torch.Tensor: Tensor of shape [num_frames, 528], containing pairwise distances for each frame.\n","    \"\"\"\n","    num_frames, num_points, _ = points_tensor.size()\n","\n","    # Generate index pairs for the upper triangle of a matrix (excluding diagonal)\n","    pairs = torch.combinations(torch.arange(num_points), r=2, with_replacement=False)  # Shape: [528, 2]\n","\n","    # Gather the coordinates for each pair of points\n","    point1 = points_tensor[:, pairs[:, 0]]  # Shape: [num_frames, 528, 3]\n","    point2 = points_tensor[:, pairs[:, 1]]  # Shape: [num_frames, 528, 3]\n","\n","    # Compute pairwise Euclidean distances\n","    pairwise_distances = torch.norm(point1 - point2, dim=2)  # Shape: [num_frames, 528]\n","\n","    return pairwise_distances\n","\n","\n","def compute_distances_and_angles_combined(points_tensor):\n","    \"\"\"\n","    Compute and concatenate pairwise distances and angles between every three points for 33 pose landmarks for each frame.\n","\n","    Args:\n","        points_tensor (torch.Tensor): Tensor of shape [num_frames, 33, 3],\n","                                       where each row is a point (x, y, z) for each frame.\n","\n","    Returns:\n","        torch.Tensor: Tensor of shape [num_frames, 528 + comb(33, 3)],\n","                      containing pairwise distances and angles for each frame.\n","    \"\"\"\n","    num_frames, num_points, _ = points_tensor.size()\n","\n","    # Precompute all pairs and triplets of indices\n","    pairs = torch.combinations(torch.arange(num_points), r=2, with_replacement=False)\n","    triplets = torch.combinations(torch.arange(num_points), r=3, with_replacement=False)\n","\n","    # Compute pairwise distances\n","    point_diffs = points_tensor[:, pairs[:, 0]] - points_tensor[:, pairs[:, 1]]  # [num_frames, num_pairs, 3]\n","    pairwise_distances = torch.norm(point_diffs, dim=2)  # [num_frames, num_pairs]\n","\n","    # Compute angles between triplets\n","    vec1 = points_tensor[:, triplets[:, 0]] - points_tensor[:, triplets[:, 1]]  # [num_frames, num_triplets, 3]\n","    vec2 = points_tensor[:, triplets[:, 2]] - points_tensor[:, triplets[:, 1]]  # [num_frames, num_triplets, 3]\n","    dot_products = torch.sum(vec1 * vec2, dim=2)  # [num_frames, num_triplets]\n","    norms = torch.norm(vec1, dim=2) * torch.norm(vec2, dim=2)  # [num_frames, num_triplets]\n","    cos_angles = dot_products / (norms + 1e-8)  # Add epsilon to avoid division by zero\n","    # angles = torch.acos(cos_angles)  # [num_frames, num_triplets]\n","\n","    # Concatenate distances and angles\n","    combined_features = torch.cat([pairwise_distances, cos_angles], dim=1)  # [num_frames, 5984]\n","    return combined_features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJ4kPYopvALi"},"outputs":[],"source":["class PreprocessedPoseVideoDataset(Dataset):\n","\n","    def __init__(self, df, num_frames, preprocessed_dir):\n","        self.df = df\n","        self.num_frames = num_frames\n","        self.preprocessed_dir = preprocessed_dir\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        # Get frontal and lateral video identifiers\n","        num_video_frontal = row['Num Video Frontal']\n","        num_video_lateral = row['Num Video Lateral']\n","        num_idx = row['NumIdx']\n","        action = row['Action']\n","\n","        # Load preprocessed frames\n","        frontal_frames = self._load_preprocessed_frames(num_video_frontal, action, num_idx)\n","        lateral_frames = self._load_preprocessed_frames(num_video_lateral, action, num_idx)\n","\n","        # Normalize images\n","        image_frontal = torch.tensor(frontal_frames, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n","        image_lateral = torch.tensor(lateral_frames, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n","\n","        label_class = torch.tensor(row['class'], dtype=torch.long)\n","        ratings = self._process_ratings(train_df_lunge, row)\n","        ratings = torch.tensor(ratings, dtype=torch.float32) if ratings is not None else None\n","\n","        # Extract pose landmarks from video frames\n","        pose_landmarks_frontal = row['front_pose']\n","        pose_landmarks_lateral = row['lat_pose']\n","        pose_landmarks_tensor_frontal = torch.tensor(pose_landmarks_frontal).float()\n","        pose_landmarks_tensor_lateral = torch.tensor(pose_landmarks_lateral).float()\n","        pose_frontal = compute_distances_and_angles_combined(pose_landmarks_tensor_frontal)\n","        pose_lateral = compute_distances_and_angles_combined(pose_landmarks_tensor_lateral)\n","\n","        return (image_frontal, image_lateral, pose_frontal, pose_lateral, label_class, ratings)\n","\n","    def _load_preprocessed_frames(self, num_video, action, num_idx):\n","        frames = []\n","        for i in range(1, self.num_frames + 1):\n","            # Construct the preprocessed file path\n","            file_name = f\"{num_video}_idx_{num_idx}_{i}.npy\"\n","            file_path = os.path.join(self.preprocessed_dir, action, file_name)\n","\n","            # Load the preprocessed .npy file\n","            frame = np.load(file_path)\n","            frames.append(frame)\n","        return np.stack(frames, axis=0)\n","\n","    def _process_ratings(self, df, row):\n","        relevant_columns = [col for col in df.columns if col.endswith('F') or col.endswith('L')]\n","        scores = row[relevant_columns].values\n","        thresholded_scores = np.where(scores >= 0.5, 1, 0)\n","        return thresholded_scores.tolist()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":809,"status":"ok","timestamp":1734906246293,"user":{"displayName":"ai research","userId":"07349987832510586956"},"user_tz":-120},"id":"DOCwP8A43hiy","outputId":"fe435d76-d377-4465-f54a-18ffb8235873"},"outputs":[{"data":{"text/plain":["103"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["train_df['Num Video Frontal'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgoxek6owRRJ"},"outputs":[],"source":["def create_dataloader(df, num_frames, batch_size=16, shuffle=True):\n","    dataset = PreprocessedPoseVideoDataset(df, num_frames, preprocessed_dir=os.path.join(base_path, 'preprocessed_images'))\n","    return DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        pin_memory=True           # Optimize for GPU training\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XO7F_HevvD7q"},"outputs":[],"source":["def train_combined_model(CustomModel, train_dataloader, eval_dataloader, epochs=1000, lr=1e-4,\n","                         device='cpu', clip_grad_norm=1.0, patience=10):\n","    # TensorBoard setup\n","    writer = SummaryWriter(log_dir=LOG_DIR)\n","\n","    CustomModel.to(device)\n","\n","    # Set up optimizer and loss functions\n","    optimizer = optim.Adam(list(CustomModel.parameters()), lr=lr)\n","    feature_loss = nn.BCEWithLogitsLoss()  # Loss for rating task (binary)\n","\n","    # Learning rate scheduler\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n","    best_eval_loss = float('inf')\n","    patience_counter = 0\n","    print(\"Training the model...\")\n","    a = 0\n","    for epoch in range(epochs):\n","        CustomModel.train()\n","        running_loss = 0.0\n","        print(f'Start of Epoch {epoch+1}')\n","        for batch_idx, batch in enumerate(train_dataloader):\n","            # Calculate progress percentage\n","            progress = (batch_idx + 1) / len(train_dataloader) * 100\n","            # Unpack batch data and move to the specified device\n","            (image_frontal, image_lateral, pose_frontal, pose_lateral, label_class, ratings) = [tensor.to(device) for tensor in batch]\n","            optimizer.zero_grad()\n","            # Forward pass through the classification model\n","            ratings_output = CustomModel(image_frontal, image_lateral, pose_frontal, pose_lateral).to(device)\n","\n","            loss = feature_loss(ratings_output, ratings)\n","\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(list(CustomModel.parameters()), clip_grad_norm)\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            # Print progress during each epoch\n","            print(f\"Epoch [{epoch + 1}/{epochs}], Progress: {progress:.2f}%, Batch [{batch_idx + 1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n","\n","        avg_loss = running_loss / len(train_dataloader)\n","\n","        # Log training loss to TensorBoard\n","        writer.add_scalar(\"Loss/Train\", avg_loss, epoch)\n","\n","        # Evaluate both models after each epoch\n","        eval_loss, hamming_distances, metrics = evaluate_combined_model(CustomModel, eval_dataloader, feature_loss, device)\n","        mean_hamming_distance = sum(hamming_distances.values()) / len(hamming_distances) if len(hamming_distances) > 0 else 0.0\n","\n","\n","        tp = metrics['TP']\n","        tn = metrics['TN']\n","        fp = metrics['FP']\n","        fn = metrics['FN']\n","\n","        # Class 1 metrics\n","        precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n","        recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n","        f1_score_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0\n","\n","        # Class 0 metrics\n","        precision_0 = tn / (tn + fn) if (tn + fn) > 0 else 0\n","        recall_0 = tn / (tn + fp) if (tn + fp) > 0 else 0\n","        f1_score_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0) if (precision_0 + recall_0) > 0 else 0\n","\n","        jaccard_index = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0.0\n","\n","        # Log validation metrics to TensorBoard\n","        writer.add_scalar(\"Loss/Validation\", eval_loss, epoch)\n","        writer.add_scalar(\"Loss/Hamming Loss\", mean_hamming_distance, epoch)\n","        writer.add_scalar(f\"Metrics/Precision\", (precision_1 +precision_0)/2, epoch)\n","        writer.add_scalar(f\"Metrics/Recall\", (recall_1 +recall_0)/2, epoch)\n","        writer.add_scalar(f\"Metrics/F1-Score\", (f1_score_1 +f1_score_0)/2, epoch)\n","        writer.add_scalar(f\"Metrics/jaccard index\", jaccard_index, epoch)\n","\n","        # Print summary for each epoch\n","        print(f\"Epoch [{epoch + 1}/{epochs}] with lr: {lr} Summary: \"\n","              f\"Train Loss: {avg_loss:.4f}, Eval Loss: {eval_loss:.4f},\"\n","              f\"Precision: {(precision_1 +precision_0)/2:.4f}, Recall: {(recall_1 +recall_0)/2:.4f},\"\n","              f\" F1-Score 0: {f1_score_0:.4f}, F1-Score 1: {f1_score_1:.4f}, F1-Score: {(f1_score_1 +f1_score_0)/2:.4f}\"\n","              )\n","        print(f\"Hamming Loss: {mean_hamming_distance:.4f}, jaccard index: {jaccard_index:.4f}\", )\n","\n","        scheduler.step(eval_loss)\n","\n","        # Early stopping and model saving\n","        if eval_loss < best_eval_loss:\n","            best_eval_loss = eval_loss\n","            patience_counter = 0\n","            torch.save(CustomModel.state_dict(), f\"best_rating_model_epoch_{epoch + 1}.pt\")\n","            print(\"Model checkpoint saved.\")\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(\"Early stopping triggered.\")\n","                break\n","\n","    writer.close()  # Close TensorBoard writer\n","    print(\"Training complete.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"toh7QNckvwN9"},"outputs":[],"source":["def evaluate_combined_model(CustomSwinTransformerModel, dataloader, feature_loss, device):\n","    \"\"\"\n","    Evaluates the model and computes evaluation metrics for each feature.\n","\n","    Args:\n","        CustomSwinTransformerModel (torch.nn.Module): The rating prediction model.\n","        dataloader (DataLoader): A DataLoader providing the evaluation data.\n","        feature_loss (nn.Module): The loss function for ratings.\n","        device (str): The device to perform evaluation on ('cpu' or 'cuda').\n","\n","    Returns:\n","        float: Average evaluation loss.\n","        dict: Hamming distance for squat features.\n","        dict: TP, TN, FP, FN for each feature.\n","    \"\"\"\n","    CustomSwinTransformerModel.eval()\n","    total_loss = 0.0\n","    num_features = FeatureNum\n","    metrics = {\n","            feature_idx: {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0} for feature_idx in range(num_features)\n","        }\n","    total_samples = {feature_idx: 0 for feature_idx in range(num_features)}\n","\n","    with torch.no_grad():\n","        a=0\n","        for batch_idx, batch in enumerate(dataloader):\n","            # Move batch data to the device\n","            print(f\"a ={a},  batch_idx = {batch_idx}\")\n","            a=a+1\n","            image_frontal, image_lateral, pose_frontal, pose_lateral, label_class, ratings = [tensor.to(device) for tensor in batch]\n","\n","            # Predict ratings using the model\n","            ratings_output = CustomSwinTransformerModel(image_frontal, image_lateral, pose_frontal, pose_lateral).to(device)\n","            total_loss += feature_loss(ratings_output, ratings)\n","\n","            # Sigmoid activation for binary predictions\n","            predicted_ratings = torch.sigmoid(ratings_output) > 0.5\n","            actual_ratings = ratings.byte()\n","\n","            # Compute TP, TN, FP, FN for each feature\n","            for feature_idx in range(num_features):\n","                preds = predicted_ratings[:, feature_idx]\n","                trues = actual_ratings[:, feature_idx]\n","\n","                metrics[feature_idx]['TP'] += (preds & trues).sum().item()\n","                metrics[feature_idx]['TN'] += (~preds & ~trues).sum().item()\n","                metrics[feature_idx]['FP'] += (preds & ~trues).sum().item()\n","                metrics[feature_idx]['FN'] += (~preds & trues).sum().item()\n","                total_samples[feature_idx] += trues.numel()\n","\n","    # Calculate Hamming distance for each feature\n","    hamming_distances = {}\n","    for feature_idx in range(num_features):\n","          hamming_distances[feature_idx] = (\n","              metrics[feature_idx]['FP'] + metrics[feature_idx]['FN']\n","          ) / total_samples[feature_idx] if total_samples[feature_idx] > 0 else 0.0\n","\n","    avg_loss = total_loss / len(dataloader)\n","\n","    total_metrics = {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0}\n","    # Print TP, TN, FP, FN for each feature\n","    for feature_idx in range(num_features):\n","        print(f\"Feature {feature_idx}:\")\n","        print(f\"  TP: {metrics[feature_idx]['TP']}\")\n","        print(f\"  TN: {metrics[feature_idx]['TN']}\")\n","        print(f\"  FP: {metrics[feature_idx]['FP']}\")\n","        print(f\"  FN: {metrics[feature_idx]['FN']}\")\n","        print(f\"  Hamming Loss: {hamming_distances[feature_idx]:.4f}\")\n","        total_metrics['TP'] += metrics[feature_idx]['TP']\n","        total_metrics['TN'] += metrics[feature_idx]['TN']\n","        total_metrics['FP'] += metrics[feature_idx]['FP']\n","        total_metrics['FN'] += metrics[feature_idx]['FN']\n","\n","    return avg_loss, hamming_distances, total_metrics\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nA8KWbG1wEIp"},"outputs":[],"source":["dataloader = create_dataloader(train_df_lunge, 16, batch_size=2)\n","eval_dataloader = create_dataloader(val_df_lunge, 16, batch_size=2)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kTNiXbXVhm0z","outputId":"4b00cce4-7290-4121-ed57-5dbf68d669b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["74\n","torch.Size([16, 3, 224, 224])\n"]}],"source":["print(len(dataloader.dataset))  # Check dataset length\n","print(dataloader.dataset[0][0].shape)  # Try accessing the first element\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":822},"executionInfo":{"elapsed":7212,"status":"ok","timestamp":1734906292222,"user":{"displayName":"ai research","userId":"07349987832510586956"},"user_tz":-120},"id":"bFrjV99Pj_EI","outputId":"f68ee9da-5926-4aff-dbcb-1c6de0621532"},"outputs":[{"data":{"application/javascript":["\n","        (async () => {\n","            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n","            url.searchParams.set('tensorboardColab', 'true');\n","            const iframe = document.createElement('iframe');\n","            iframe.src = url;\n","            iframe.setAttribute('width', '100%');\n","            iframe.setAttribute('height', '800');\n","            iframe.setAttribute('frameborder', 0);\n","            document.body.appendChild(iframe);\n","        })();\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Clear previous TensorBoard runs if needed\n","!rm -rf /content/drive/MyDrive/Vision_GYM_Research/tensorboard_logs\n","\n","# Start TensorBoard in Colab\n","%load_ext tensorboard\n","%tensorboard --logdir $LOG_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7McUY4S8kFtu"},"outputs":[],"source":["%tensorboard --logdir $LOG_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guEKv3l43oAz"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"zF3zzRZuwZ-R","outputId":"7a6935be-6769-4a1e-dfe8-93ef1954ee2d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["cpu\n","Training the model...\n","Start of Epoch 1\n","Epoch [1/1000], Progress: 2.70%, Batch [1/37], Loss: 0.7046\n"]}],"source":["# Initialize the model\n","rating_model = MultiModalModel()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(device)\n","\n","# Train the model\n","train_combined_model(rating_model, dataloader, eval_dataloader, epochs=1000, lr=4e-4, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_3jUKGgCZ1f"},"outputs":[],"source":["train_combined_model(rating_model, dataloader, eval_dataloader, epochs=1000, lr=1e-4, device=device)"]},{"cell_type":"markdown","metadata":{"id":"eHNerauVcMWY"},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}